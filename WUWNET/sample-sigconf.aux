\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{baggeroer1993overview}
\citation{niu2017source}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks Based Source Localization}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks Models and Function Approximation}{1}{subsection.2.1}}
\citation{bishop2006pattern}
\citation{niu2017source}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Regularization for neural networks}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training the neural networks with sparse constraint}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulation and experimental results}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Parameter settings}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Weights summaries in hidden layer(left:no constraint,right:with constraint). Sparse constraint training makes the weight coefficient show the group structure,either all zero,or basic is not zero.\relax }}{3}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The effect of sparse constraint training}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model accuracy and Average-activation-density for different $\lambda $. Regularization on neuron-level significantly reduces the average activation density without much loss in model accuracy.\relax }}{3}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The learned sparse representation model. The learned feature space spans data(scm) space likelihood that few basis functions $\phi $ explain a given data.\relax }}{3}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Comparison with conventional matched-field processing method}{3}{subsection.3.3}}
\citation{tolstoy1989sensitivity,feuillade1989environmental,del1988effects}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Localization accuracy of FNN and MFP on SWell96Ex-S5 data\relax }}{4}{table.caption.8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{my-label}{{1}{4}{Localization accuracy of FNN and MFP on SWell96Ex-S5 data\relax \relax }{table.caption.8}{}}
\newlabel{my-label@cref}{{[table][1][]1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Absolute mean error of FNN and MFP on SWell96Ex-S5 data(m)\relax }}{4}{table.caption.9}}
\newlabel{my-label}{{2}{4}{Absolute mean error of FNN and MFP on SWell96Ex-S5 data(m)\relax \relax }{table.caption.9}{}}
\newlabel{my-label@cref}{{[table][2][]2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots of sound speed profiles.\relax }}{4}{figure.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The influences of ssp mismatch on FNN classifier}{4}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces FNN positioning performance curve on simulation data(frequency:109,232,385Hz). FNN is also sensitive to ssp mismatch, but still performs better than MFP. \relax }}{4}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces FNN positioning performance curve on simulation data. FNN model tolerance can be by significantly improved by mixed data training\relax }}{4}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Increase model tolerance by data-model mixed training}{4}{subsection.3.5}}
\bibstyle{unsrt}
\bibdata{acmart}
\bibcite{baggeroer1993overview}{{1}{}{{}}{{}}}
\bibcite{niu2017source}{{2}{}{{}}{{}}}
\bibcite{goodfellow2016deep}{{3}{}{{}}{{}}}
\bibcite{bishop2006pattern}{{4}{}{{}}{{}}}
\bibcite{tolstoy1989sensitivity}{{5}{}{{}}{{}}}
\bibcite{feuillade1989environmental}{{6}{}{{}}{{}}}
\bibcite{del1988effects}{{7}{}{{}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.62497pt}
\newlabel{tocindent2}{11.81937pt}
\newlabel{tocindent3}{0pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{5}{section.4}}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{5}{section*.14}}
\@writefile{toc}{\contentsline {section}{References}{5}{section*.16}}
\newlabel{TotPages}{{5}{5}{}{page.5}{}}
