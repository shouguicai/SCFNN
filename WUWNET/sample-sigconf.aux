\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{roan2009distributed}
\citation{erling2007performance}
\citation{tenney1984distributed,bar1995multitarget,ristic2001influence,kaplan2001bearings}
\citation{schweppe1968sensor}
\citation{wax1985decentralized}
\citation{tolstoy1993matched,baggeroer1988matched,baggeroer1993overview}
\citation{nichols2015cross,tollefsen2017multiple}
\citation{Yang2015Issues}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Machine Learning Based Data Sparse Representation}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks Models and Function Approximation}{1}{subsection.2.1}}
\citation{niu2017source}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Regularization for neural networks}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Learn useful sparse representations from data}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulation and experimental results}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Parameter Settings}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Weights summaries in hidden layer(left:no constraint,right:with constraint). sparse constraint training makes the weight coefficient show the group structure,either all zero,or basic is not zero.\relax }}{3}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Effect of Sparse Constraint Training}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model error and Average-activation-density for different $\lambda $. Regularization on neuron-level significantly reduces the average activation density without much cost in model error.\relax }}{3}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The learned sparse representation model.The learned feature space spans data(scm) space likelihood that few basis functions $\phi $ explain a given data.\relax }}{3}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Comparison with conventional matched-field processing method}{3}{subsection.3.3}}
\citation{tolstoy1989sensitivity,feuillade1989environmental,del1988effects}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Localization accuracy of FNN and MFP on SWell96Ex-S5 data\relax }}{4}{table.caption.8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{my-label}{{1}{4}{Localization accuracy of FNN and MFP on SWell96Ex-S5 data\relax \relax }{table.caption.8}{}}
\newlabel{my-label@cref}{{[table][1][]1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Absolute mean error of FNN and MFP on SWell96Ex-S5 data(m)\relax }}{4}{table.caption.9}}
\newlabel{my-label}{{2}{4}{Absolute mean error of FNN and MFP on SWell96Ex-S5 data(m)\relax \relax }{table.caption.9}{}}
\newlabel{my-label@cref}{{[table][2][]2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots of sound speed profiles.\relax }}{4}{figure.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The influences of ssp mismatch on FNN classifier}{4}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces FNN positioning performance curve on simulation data(frequency:109,232,385Hz). FNN is also sensitive to ssp mismatch, but still performs better than MFP. \relax }}{4}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces FNN positioning performance curve on simulation data. FNN model tolerance can be by significantly improved by mixed data training\relax }}{4}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Increase model tolerance by mixed data training}{4}{subsection.3.5}}
\bibstyle{unsrt}
\bibdata{acmart}
\bibcite{roan2009distributed}{{1}{}{{}}{{}}}
\bibcite{erling2007performance}{{2}{}{{}}{{}}}
\bibcite{tenney1984distributed}{{3}{}{{}}{{}}}
\bibcite{bar1995multitarget}{{4}{}{{}}{{}}}
\bibcite{ristic2001influence}{{5}{}{{}}{{}}}
\bibcite{kaplan2001bearings}{{6}{}{{}}{{}}}
\bibcite{schweppe1968sensor}{{7}{}{{}}{{}}}
\bibcite{wax1985decentralized}{{8}{}{{}}{{}}}
\bibcite{tolstoy1993matched}{{9}{}{{}}{{}}}
\bibcite{baggeroer1988matched}{{10}{}{{}}{{}}}
\bibcite{baggeroer1993overview}{{11}{}{{}}{{}}}
\bibcite{nichols2015cross}{{12}{}{{}}{{}}}
\bibcite{tollefsen2017multiple}{{13}{}{{}}{{}}}
\bibcite{Yang2015Issues}{{14}{}{{}}{{}}}
\bibcite{goodfellow2016deep}{{15}{}{{}}{{}}}
\bibcite{niu2017source}{{16}{}{{}}{{}}}
\bibcite{tolstoy1989sensitivity}{{17}{}{{}}{{}}}
\bibcite{feuillade1989environmental}{{18}{}{{}}{{}}}
\bibcite{del1988effects}{{19}{}{{}}{{}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.62497pt}
\newlabel{tocindent2}{11.81937pt}
\newlabel{tocindent3}{0pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{5}{section.4}}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{5}{section*.14}}
\@writefile{toc}{\contentsline {section}{References}{5}{section*.16}}
\newlabel{TotPages}{{5}{5}{}{page.5}{}}
