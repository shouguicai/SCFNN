\section{Introduction}
As the increasing number of manned platforms deploying receiver arrays or  fixed/mobiled nodes in distributed underwater sensor networks,we are facing the scene of underwater big data.Due to the limitation of energy used,it is not realistically to process all the incoming data\cite{Yang2015Issues}.Now,machine learning technology is more and more mature, we should try to introduce some smart into our system,to extract task-relevent information,discover new patterns,construct a useful representation.It is convenient for us to share and transfer information between different platforms once the data can be reduced to small useful finite sets.

\section{Machine Learning Based Data Sparse Representation}
In Underwater Networks,we will get a lot of data. What we really care about,is not the signal itself,but the information contained in the siganl.Meanwhile,the communication for underwater sensors is costly.So develop methods to represent the information present in metadata with a sparse format is very meaningful.

In this section,we discuss an effective scheme using machine learning based methods.

\subsection{Neural Networks Models and Function Approximation}
As we known,neural networks models can be viewed as a mathematical function $f$.Taking feedforward neural network(FNN) as an example,they define a mapping ${\rm{y}}=f(x;\theta )$ between input $x$ and output $y$ by parameter $\theta$,which needed to be learned by a rule.Feedforward networks are called networks because they are typically represented by composing together many different functions.We might have two function $f^{1}$, $f^{2}$ connected in a chain\cite{goodfellow2016deep}, to form
$f(x) = f^{2}(f^{1}(x))$.

FNN extend linear models to represent nolinear transformed format $\phi(x)$ of input $x$.The transform function $\phi$ can be thinked as providing a set of features describing $x$, or as providing a new representation for x.The key problem here is how to choose the mapping $\phi$.

The strategy of machine learning is to learn $\phi$.In a feedforward network,$\phi$ defines a hidden layer $h=\phi(x;w^{(1)})$,then the total model is $y=f(x;\theta,w)=hw^{(2)}$.
obviously,We need to learn $\phi$ from a broad class of functions,and parameters $w$ mapping $\phi(x)$ to the desired output.

In most cases,our parametric model defines a distribution $p(y|x;\theta)$.Simply, we can use the principle of maximum likelihood to learn parameters in model.
\begin{equation}
J(\theta ) =  - {E_{x,y \sim {p_{data}}}}\log {p_{\bmod el}}(y|x)
\end{equation}
where the specific form of $p_{\bmod el}$ is defined by networks.
As maximum likelihood is a consistent estimator, the model is capable of representing the training distribution.

\subsection{Regularization for neural networks}

There are two main kind of regularization strategy for neural networks,one is weight-level regularization,another is neuron-level regularization with activation penalty.
\begin{equation}
\tilde J(\theta ;X,y) = J(\theta ;X,y) + \alpha \Omega (\theta )+ \beta \Omega (h)
\end{equation}
where $\Omega (\theta )$ is parameter norm penalty, $\Omega (h)$ is penalty on the activations of the units,$\alpha$,$\beta$ are hyperparameters that weight the relative contribution of
the norm penalty term.Weight decay term penalize the size of the model parameters,while,the activation penalty term encouraging their activations to be sparse.

\subsection{Learn useful sparse representations from data}
When a useful sparse representation of any given data is learned,each datum will then be encoded as a sparse code,thus we can use the least possible amount of resource to store or transfer the data.
\begin{equation}
\begin{split}
&\tilde J(\theta ){\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ = }}{\kern 1pt} {\kern 1pt} {\kern 1pt}  - {E_{x,y \sim {p_{data}}}}\log {p_{\bmod el}}(y|x){\rm{ + }}{\kern 1pt} {\kern 1pt} \lambda {\left\| h \right\|_1}\\
& s.t.{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\left\| {{W_i}^{(1)}} \right\|_2} \le C{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} \forall {\kern 1pt} i = 1, \cdots ,M\\
\end{split}
\end{equation}
The sparsity here not only indicates sparse data,but also indicates that the model features sparse. Thus,we use $L1$ norm to promote sparse neurons activations, and constrain the norm of each column of the weight matrix to prevent any one hidden unit from having very large weights. In the equation, $M$ is the number of neurons in hidden layer.

\section{Simulation and experimental results}
A notable recent example of using machine learning method in underwater acoustic is the application of nonlinear classification to source localization\cite{niu2017source}.
In this section,we implement a simple FNN with just one hidden layer to learn source range directly from observed acoustic data, 
and compare the performance of the classifier with the traditional matching field processing method (Bartlett) in terms of simulation data and experimental data, respectively.In addition, we briefly discuss the influence of sound speed profile(ssp) mismatch on the performance of FNN classifier and improve the generalization of the classifier by training the model using data sampled under different ssp.

Simulation environment is the widely studied SWell96Ex test,conducting in a 216m deep shallow waveguide environment.The ship proceeded northward at a speed of 2.5 m/s.

The source ship has two sound source, a deep source (J-15) and a shallow source (J-13).In all the simulations, we used the shallow sound source,
which was towed at a depth of about 9m and transmitted 9 frequencies between 109Hz and 385Hz.

\subsection{Parameter Settings}
In simulation part,acoustic data used to train and test the neural network is generated by kraken.Snapshot $N_{s}$ is 10,number of vertical array elements $L$ is 21,input layer neurons $D$ of FNN is $L^{2} \times N_{fre}$(number of frequency used),number of neurons in the output layer (number of classes) $K = 300$.We just give the same amount of dimensions the original data are encoded in to provide enough dimensions to learn over-complete features at first,which means neurons in hidden layer $M = D$. Specifically,the cost function now is
\begin{equation}
\begin{split}
&\min {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} \tilde J(\theta ){\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ = }}{\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ - }}{{\rm{1}} \over N}\sum\limits_{n = 1}^N {\sum\limits_{k = 1}^K {{t_{nk}}\ln {y_{nk}}{\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ + }}{\kern 1pt} {\kern 1pt} \lambda {{\left\| h \right\|}_1}} }\\
& s.t.{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\left\| {{W_i}^{(1)}} \right\|_2} \le C{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} \forall {\kern 1pt} i = 1, \cdots ,M\\
\end{split}
\end{equation}
where $t_{nk}$ and $y_{nk}$ are predictive and real probability of sample data $x$ belongs to class $k$ separately.Moreover we will choose $C=1$ here.
For the sake of learning speed ,we use the $ReLU$ activation in hidden layer and use the $softmax$
function in the output layer because this case is a multiple class problem.
The training set is 3000 samples of uniform sampling between 1.82\--8.65km,test set is another 300 data samples sampling from the same range.The noise in the simulation is set to complex gaussian white noise.

Experimental data gets from SWell96Ex Event S5,we use the receive sound pressure of VLA to train our neural network.The array recorded a total of 75 min of data,In order to facilitate processing, we took 0\--50min data as a training set.

Consistent with the simulation part,we divide the trajectory into 300 grids,25m each.We set 1 second as a snapshot and get 3000 sample covariance matrix (SCM),the sample covariance matrix is averaged at every two snapshots.
At the time of training, we took 9/10(2700)of samples as training set and another 1/10(300) as test set.

\subsection{The Effect of Sparse Constraint Training}
Compared to the case of training without sparse constraints, sparse constraint makes the weight coefficient in the hidden layer show the group structure,either all zero, or basic is not zero.In the meantime, the number of feature vectors reduced from 1500 to 140.In addition, it can be seen that the learned weights is related to the frequency,even at the same frequency, the the weight corresponding to real and imaginary parts is also different.

\begin{figure}
\includegraphics[width=4cm,height=3cm]{figure/Weights_summaries_in_hidden_laye_swell_exp}
\includegraphics[width=4cm,height=3cm]{figure/Weights_summaries_in_hidden_laye_swell_exp_sc_3dot5_e_neg_5}
\caption{weights summaries in hidden layer(left:no constraint,right:with constraint).}
\end{figure}

\begin{figure}
\includegraphics[width=4cm,height=3cm]{figure/Accuracy_on_Test_data_for_different_constraint_exp}
\includegraphics[width=4cm,height=3cm]{figure/hidden-Average-Density-on-Test-data-for-different-constraint_exp}
\caption{Accuracy and Average-Density on test data for different-constraint.}
\end{figure}

\begin{figure}
\includegraphics[width=4cm,height=3cm]{figure/selected_16_features_real_part}
\includegraphics[width=4cm,height=3cm]{figure/selected_16_features_imag_part}
\caption{Selected 16 features learned by neural network.}
\end{figure}

Apart from being beneficial to feature selection,sparse constraint reduce the activation rate of neurons in the hidden layer without reduce accuracy. As the Fig.2 shows, when the coefficient is 3.5e-5, the average activation neuron number is 11,which is greatly reduced,the activation rate is only 0.7{\%}.
These mean the input 1323-dimension data can be represented by 140 feature vectors, and each data sample can be represented by only 11 feature.To sum up, by using regularization strategy on neural networks level and weight-level of the FNN, we can get a sparse and low rank model, sparse means the transfered represention is sparse, low rank means the rank of learned weigth matrix is low. 

\subsection{Comparison with traditional matching field processing method}
As a reference,here we use Bartlett Processor to position the source position.There are two kinds of replica-field used in Bartlett Processor,one is calculated from model by kraken(noted as bartlett 2), another is from measurement data(noted as bartlett 1).
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\caption{Localization accuracy of FNN and MFP on SWell96Ex-S5 data}
\label{my-label}
\begin{tabular}{@{}lllll@{}}
\toprule
Methods       & FNN    & MCE    & Bartlett 1 & Bartlett 2 \\ \midrule
109Hz         & 89.3\% & 72.3\% & 37.7\%     & 3.7\%      \\
232Hz         & 97\%   & 91\%   & 17.7\%     & 4.3\%      \\
385Hz         & 99.7\% & 97.7\% & 14\%       & 0.67\%     \\
109,232,385Hz & 99\%   & 99.7\% & 40.7\%     & 7.7\%      \\ \bottomrule
\end{tabular}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\caption{Absolute mean error of FNN and MFP on SWell96Ex-S5 data(m)}
\label{my-label}
\begin{tabular}{@{}lllll@{}}
\toprule
Methods       & FNN  & MCE   & Bartlett 1 & Bartlett 2 \\ \midrule
109Hz         & 28.1 & 290.3 & 852.8      & 1219.5     \\
232Hz         & 7.4  & 2.5   & 832.3      & 832.3      \\
385Hz         & 0.08 & 0.58  & 1266.7     & 1756.3     \\
109,232,385Hz & 0.25 & 0.083 & 477.2      & 722.9      \\ \bottomrule
\end{tabular}
\end{table}

The accuracy and absolute error of methods under different frequency is summed in table 1 and table 2.As we can see,whether it is single frequency or combination frequency,the accuracy of FNN is always better than Bartlett,and not worse than direct data match(noted as MCE).FNN is helpful in positioning problem.

\subsection{The influences of ssp mismatch on FNN classifier}
In the MFP, the model accuracy is heavily affected by the mismatch.Fig.5 gives the FNN positioning results in different degrees change of sound speed profile. Here, snapshot is 10, SNR is 5dB.
\begin{figure}
\includegraphics[width=6cm,height=5cm]{figure/ssp3}
\caption{plot of sound speed profile.}
\end{figure}
Comparing to optimized-ssp,the i905-ssp has only a very small change,within 0.5m/s at the same depth.The change in i906-ssp is much significant,whic can be seen from the shape in Fig.4.
\begin{figure}
\includegraphics[width=4cm,height=3cm]{figure/Accuracy_to_SNR_FNN_vs_Bartlett_MCE}
\includegraphics[width=4cm,height=3cm]{figure/Error_to_SNR_FNN_vs_Bartlett_MCE}
\includegraphics[width=4cm,height=3cm]{figure/Accuracy_to_SNR_FNN_vs_Bartlett_MCE_i906}
\includegraphics[width=4cm,height=3cm]{figure/Error_to_SNR_FNN_vs_Bartlett_MCE_i906}
\caption{FNN positioning performance curve(frequency:109,232,385Hz).}
\end{figure}

Fig.5 plots the performance curves for FNN, Bartlett, MCE by 1000 times Monte Carlo simulations.When the change in ssp is relatively small, FNN positioning best,MCE second and Bartlett worst.When the shape of ssp change,the accuracy
order unchanged,but the absolute error of FNN becomes bigger than MCE.


\subsection{Co-training using data collected from different ssp}
The simulation results show that the hybrid ssp data training can significantly improve the generalization ability of the classifier,which means FNN can learn weigths over a set of changing ssp.This is an interesting and useful discovery.

%\begin{figure}
%\includegraphics[width=4cm,height=3cm]{figure/combinevssingle_lef}
%\includegraphics[width=4cm,height=3cm]{figure/combinevssingle_right}
%\caption{Comparison of mixed data training and single data training .}
%\end{figure}

In Fig.6, i906{*} is little changed from i906,for the sake of testing.Although the accuracy for i905 has a little glissade compared with single data training,the performance for i906 imporved.In general,the trained FNN classifier works well on both two different shape ssp.

\begin{figure}
\includegraphics[width=4cm,height=3cm]{figure/Accuracy_to_SNR_Combined_vs_Single}
\includegraphics[width=4cm,height=3cm]{figure/Error_to_SNR_Combined_vs_Single}
\caption{FNN positioning performance curve.}
\end{figure}

\section{Conclusions}
%In a recent article\cite{niu2017source}, Niu considers the problem of sound source location in marine waveguides as a classification problem under the framework of machine learning,and verified the ideal on the Noise09 experimental data.
From the veiw of information theory, the objective of signal processing is to exact specific task-relevant information from data.
How to exact the relevant information from measured data is an important issue.Machine learning may be able to provide ideas for this issue, because of its strong ability to learn and characterize.
In this paper,we develop a method to learn sparse representation from data based on neural networks.
Verification on SWell96Ex experimental data shows this method can help select feature vectors and extract task-related information contained in the siganl. The pretained sparse repesentation model can help the underwater networks use the least possible amount of resource to store or transfer the data. Compared to the traditional MFP method,the neural network trained model have more ability to resist sound speed profile mismatch and can fine-tune to a feature extractor conveniently.

\begin{acks}
The work is supported by LALAL

\end{acks}
